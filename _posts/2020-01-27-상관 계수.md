---
title:        Correlation Coefficients
date:         2020-01-27
category:     데이터 분석
comments:     true
---

## 상관계수

상관계수(correlation coefficient)란 두 변수가 ordinal scale로 주어졌을 때 둘 사이에 얼마나 상관관계가 있는지 측정하는 척도이다. 보통 양의 상관관계를 가질 때 상관계수가 크고, 음의 상관관계를 가질 때 상관계수가 작도록 설계되어 있다. 여기서 상관관계가 반드시 "선형적이다"의 의미를 포함하진 않지만 가장 많이 사용되는 피어슨 상관계수가 이 선형성을 측정하고 있다.

상관계수는 상관분석 및 회귀분석에서 결정계수를 계산할 때 사용된다. 추천시스템의 협력 필터링에서 두 아이템 혹은 두 고객의 유사도를 계산할 때 추천이 얼마나 성공적인지 계산하는 평가함수로 이용되기도 한다.

---

### 1. 피어슨(Pearson) 상관계수

두 변수 X와 Y에 대해 각각 3개의 아래와 같은 관측치가 있다고 하자. 우선 본론으로 들어가기 전에 유의할 점이 있다. 기하학적으로 $X$와 $Y$는 각각의 차원을 이루며, 각 관측치가 하나의 점이라고 생각하면 편할 것이다. 즉, 2차원 평면에서의 3개의 점이 놓여있는 것이다. 하지만 우리의 관심 대상은 변수 사이의 관계이므로 이번만은 $X$와 $Y$가 관측치의 개수만큼의 길이를 갖는 수열이라 생각하는 것이 편하다.

$$ X = (1, 2, 3)\\ Y = (4, 5, 6) $$

우리는 두 변수 $X$와 $Y$에 따라 다음을 만족하는 함수 $\rho$를 만들어 사용하고 싶다.

- $X$가 커짐에 따라 $Y$가 커지면 높은 값의 $\rho$를 가짐
- $X$와 $Y$의 상관관계가 없으면 낮은 값의 $\rho$를 가짐
- $X$가 커짐에 따라 $Y$가 작아지면 훨씬 더 낮은 값의 $\rho$를 가짐

주어진 $X$와 $Y$를 벡터라고 생각하면, 우리는 위의 세가지를 모두 충족시키는 척도를 이미 알고 있다. 바로 **내적(inner product)**이다. 실제로 내적을 계산해보면 다음을 얻게된다.

$$\langle X, Y \rangle = (1 \times 4) + (2 \times 5) + (3 \times 6) = 32$$

32라는 값은 $X$와 $Y$의 나열 순서를 달리할 때 얻을 수 있는 가장 큰 내적 값이다. 무슨말이냐하면, $X$를 고정하고 $Y = (6, 5, 4)$로 순서를 섞으면 내적값은 반드시 32보다 작다.

$$\langle X, Y \rangle = (1 \times 6) + (2 \times 5) + (3 \times 4) = 28 < 32$$

어떻게 순서를 나열하더라도 32라는 값을 넘기지 못하는 이러한 성질은 간단한 귀납법으로 증명할 수 있다.

---
**_proof_**

Let $X = (x_1, x_2, \ldots, x_n)$ and $Y = (y_1, y_2, \ldots, y_n)$ be given. Assume that $X$ is increasing. We will show that the inner product $\langle X, Y \rangle = \sum\limits_{i=1}^n (x_i \cdot y_i)$ has the maximum value when $Y$ is also increasing. In other words, $Y_0 = \underset{Y}{\arg\max} \langle X, Y \rangle$ if and only if $Y_0$ is increasing.

Suppose that $y_k = \max\limits_j y_j$. Note that $x_n = \max\limits_i x_i$. For any two indices $i\neq n$ and $j\neq k$, consider two equations

$$x_n \cdot y_k + x_i \cdot y_j$$

$$x_n \cdot y_j + x_i \cdot y_k.$$

By the following,

$$
\begin{aligned}
(x_n \cdot y_k + x_i \cdot y_j&) - (x_n \cdot y_j + x_i \cdot y_k) 
\\ &= x_n \cdot (y_k- y_j) - x_i \cdot (y_k - y_j)
\\ &= (x_n - x_i) \cdot (y_k - y_j) \geq 0
\end{aligned}
$$

we obatin $x_n \cdot y_n + x_i \cdot y_j \geq x_n \cdot y_j + x_i \cdot y_n$ (equality holds only when $x_n = x_i$ and $y_k = y_j$). Since $x_i$ and $y_j$ are arbitrary, $x_n$ should be multiplied with $y_k = \max\limits_j y_j$.

Hence we may assume $n = k$ and so $\max\limits_Y \langle X, Y \rangle = x_n \cdot y_n + \max\limits_{Y'}\langle X', Y' \rangle$, where $X' = (x_1, x_2, \ldots, x_{n-1})$ and $Y' = (y_1, y_2, \ldots, y_{n-1})$. Then our claim will be done recursively.

---

하지만 단순히 내적을 취하는 방식은 변수가 달라질 때마다 전혀 다른 기준치를 갖게 된다. 예를 들어, 

$$ X = (1, 2, 3)\\ Y_1 = (4, 5, 6)\\ Y_2 = (-2, -1, 0) $$

이라고 하자. 여기서 

$$X = Y_2 + 3 \\ Y_1 = X + 3$$

이라는 동일한 선형 관계를 가짐에도 불구하고 각각의 내적은 다른 값을 갖게 된다.

$$ \rho(X, Y_1) = \langle X, Y_1 \rangle = 1\cdot 4 + 2\cdot 5 + 3\cdot 6 = 32 $$

$$ \rho(X, Y_2) = \langle X, Y_2 \rangle = 1\cdot(-2) + 2\cdot(-1) + 3\cdot 0 = -4 $$

이러한 문제점을 해결하기 위해 각각의 변수에 대한 **평균** 혹은 **기대값** $\mu$를 계산하여 빼주는 방법이 있다. 

$$\rho(X, Y) := \langle X - \mu(X), Y - \mu(Y) \rangle$$

이렇게하면 $X$와 $Y$가 실제로 선형적인 관계일 때, 기존의 내적보다는 개선된 상관계수의 역할을 한다.

---
**_proof_**

Suppose $Y = aX + b$. By the linearity of expectation, $\mu(Y) = a\mu(X) + b$. Remind that 

$$a\mu(X) = \mu(Y) - b.$$

If we plug in $X-\mu(X)$ and $Y-\mu(Y)$ instead of $X$ and $Y$, we obtain

$$
\begin{aligned}
a\big(X - \mu(X)\big) + b &= (aX + b) - a\mu(X)
\\ &= Y - \big(\mu(Y) - b\big)
\\ &= \big(Y - \mu(Y)\big) + b.
\end{aligned}
$$

Hence

$$
Y - \mu(Y) = a\big(X - \mu(X)\big).
$$

Now, $b$ is not considerable anymore! 

---

위의 방법으로 평균을 뺀 변수 $X$, $Y_1$, $Y_2$ 사이의 상관계수를 구하면 다음과 같다.

$$ X = Y_1 = Y_2 = (-1, 0, 1) $$

$$ \langle X, Y_1 \rangle = \langle X, Y_2 \rangle = 2 $$

만약 두 변수가 $Y = aX + b$를 만족한다면, 각 변수의 평균을 빼줌으로써 $b = 0$이라고 가정할 수 있게 된다. 이때 우리의 상관계수는 다음과 같이

$$ \rho(X, Y) = \langle X, Y \rangle = \langle X, aX \rangle = a\left\lVert X \right\rVert^2 $$

여전히 $\rho$는 기울기 $a$ 및 변수의 크기에 영향을 받고 있다. 예를 들어

$$ X = (-1, 0, 1)\\ Y_1 = (-2, 0, 2)\\ Y_2 = (-3, 0, 3) $$

라 하면, 평균이 모두 $0$이고 서로 양의 선형관계를 보이지만 $\rho$값이 다르다.

$$ \rho(X, Y_1) = \langle X, Y_1 \rangle = (-1)\cdot (-2) + 1\cdot 2 = 4 $$

$$ \rho(X, Y_2) = \langle X, Y_2 \rangle = (-1)\cdot (-3) + 1\cdot 3 = 6 $$

이 문제는 각각의 변수를 **정규화(normalization)**하여 해결할 수 있다.

$$
\rho(X, Y) := \langle \frac{
  X - \mu(X)
}{
  \left\lVert X - \mu(X) \right\rVert
}, \frac{
  Y - \mu(Y)
}{
  \left\lVert Y - \mu(Y) \right\rVert
} \rangle
$$

혹은

$$ \rho(X, Y) := \frac{1}{n-1} \langle \frac{X - \mu(X)}{\sigma(X)}, \frac{Y - \mu(Y)}{\sigma(Y)} \rangle $$

---
**_proof_**

Since we assume $\mu(X) = 0$,

$$\sqrt{n-1}\cdot\sigma(X) = \sqrt{\sum\limits_{i=1}^n x_i^2} = \left\lVert X \right\rVert.$$

Similarly, $\sqrt{n-1}\cdot\sigma(Y) = \left\lVert Y \right\rVert $.

If $a \neq 0$,

$$
\begin{aligned}
a \big(\frac{X}{\left\lVert X \right\rVert}\big) 
&= a \cdot \frac{|a|X}{|a| \left\lVert X \right\rVert}
\\ &= |a| \cdot \frac{aX}{\left\lVert aX \right\rVert}
\\ &= |a| \big(\frac{Y}{\left\lVert Y \right\rVert}\big).
\end{aligned}
$$

Hence

$$
\frac{Y}{\left\lVert Y \right\rVert} = \frac{a}{|a|} \cdot \frac{X}{\left\lVert X \right\rVert}.
$$

---

여기서 우리는 다음 두 가지 중요한 점을 관찰할 수 있다.

1. $\rho$는 두 변수를 표준화(Standardization)한 후, 코사인 유사도를 구한 것과 같다.
2. $Y = aX+b$를 만족할 때, $\rho(X, Y) = \text{sgn}(a)$ 이다.

두 변수가 $X = (x_1, x_2, \ldots, x_n)$, $Y = (y_1, y_2, \ldots, y_n)$일 때, $\rho$를 풀어쓰면 다음과 같은 식을 얻을 수 있다.

$$
\rho(X, Y) = \frac
{
  \sum\limits_{i=1}^n \big(x_i - \mu(X)\big) \cdot \big(y_i - \mu(Y)\big)
}
{
  \sqrt{
    \sum\limits_{i=1}^n \big(x_i - \mu(X)\big)^2
  }
  \cdot \sqrt{
    \sum\limits_{i=1}^n \big(y_i - \mu(Y)\big)^2
  }
}
$$

우리는 이러한 함수 $\rho$를 **피어슨 상관계수**라고 부른다. 피어슨 상관계수는 두 unit vector가 이루는 각도의 cosine 값이기 때문에 $-1$에서 $1$ 사이의 값을 가진다. (이는 Cauchy-Schwarz 부등식으로도 증명 가능하다.)

피어슨 상관계수는 결과적으로 다음 세 조건을 만족한다.

- $X$가 커짐에 따라 $Y$가 커지면 $\rho$가 $1$에 가까워짐
- $X$와 $Y$의 상관관계가 없으면 $\rho$가 $0$에 가까워짐
- $X$가 커짐에 따라 $Y$가 작아지면 $\rho$가 $-1$에 가까워짐

---

### 2. 스피어만(Spearman) 상관계수

**스피어만 상관계수**는 피어슨 상관계수로부터 탄생한 함수이다. 우선 피어슨 상관계수 대신에 스피어만 상관계수를 사용하는 목적을 살펴보자.

1. 피어슨 상관계수는 변수 $X$와 $Y$가 ordinal scale임을 가정한다. 스피어만 상관계수는 특히 discrete ordinal scale인 경우에 적용할 수 있다.

2. 스피어만 상관계수는 **선형성**에 덜 민감하다. 두 변수가 $Y = 2X+1$를 만족하든 $Y = e^X$를 만족하든 피어슨 상관관계는 다르지만 스피어만 상관계수는 같다.

스피어만 상관계수의 정의는 우선 변수들의 **rank ordering**를 계산하는데서 시작한다. 다음 예를 보자.

$$ X = (5, -3, 4, 1) \Rightarrow \text{rk}(X) = (1, 4, 2, 3) $$

$X$에서 관측치 $5$가 가장 크기 때문에 $1$이라는 rank를 가진다. 그다음 $-3$은 가장 작은 수이므로 rank 값은 $4$이다. SQL에서의 RANK() 함수와 동일하다. 이제 크기가 $n$인 두 변수 $X$와 $Y$가 있을 때 스피어만 상관계수는 다음과 같이 정의된다.

$$ \rho_s := 1 - \frac{6 \cdot \left\lVert rk(X) - rk(Y) \right\rVert^2}{n(n^2-1)} $$

$rk(X)$와 $rk(Y)$의 $i$번째 관측치 사이의 rank 차이를 $d_i$라 하여 식을 표현하기도 한다.

$$ \rho_s := 1 - \frac{6 \cdot \sum\limits_{i=1}^n d_i^2}{n(n^2-1)} $$

스피어만 상관계수는 피어슨 상관계수와 굉장히 달라보이지만 실은 변수를 rank로 바꾼 후 피어슨 상관계수를 계산한 것과 같다.

---

**_proof_**

Let both $X$ and $Y$ have size $n$. We may assume that both $X$ and $Y$ are already updated by the $rk$ function. That is, 

$$X \leftarrow \text{rk}(X) 
\\ Y \leftarrow \text{rk}(Y)$$

Recall

$$
\rho(X, Y) = \frac
{
  \sum\limits_{i=1}^n \big(x_i - \mu(X)\big) \cdot \big(y_i - \mu(Y)\big)
}
{
  \sqrt{
    \sum\limits_{i=1}^n \big(x_i - \mu(X)\big)^2
  }
  \cdot \sqrt{
    \sum\limits_{i=1}^n \big(y_i - \mu(Y)\big)^2.
  }
}
$$

Since $X$ and $Y$ are permutations of $ \[n\] = \{ 1, 2, \ldots, n \} $, we get 

$$
\sum\limits_{i=1}^n x_i 
= \sum\limits_{i=1}^n i = \frac{n(n+1)}{2}
= \sum\limits_{i=1}^n y_i
$$

and

$$
\sum\limits_{i=1}^n x_i^2 
= \sum\limits_{i=1}^n i^2 = \frac{n(n+1)(2n+1)}{6} 
= \sum\limits_{i=1}^n y_i^2 .
$$

Note that
$$ \mu(X) = \frac{1}{n} \sum\limits_{i=1}^n x_i = \frac{n+1}{2} = \mu(Y). $$

Hence

$$
\begin{aligned}
r_s(X, Y) &= \frac{
  \sum\limits_{i=1}^n \big(x_i - \mu(X)\big) \cdot \big(y_i - \mu(Y)\big)
}{\sqrt{
  \sum\limits_{i=1}^n \big(x_i - \mu(X)\big)^2
  } \cdot \sqrt{
  \sum\limits_{i=1}^n \big(y_i - \mu(Y)\big)^2
  }
}
\\ &= \frac{
  \sum\limits_{i=1}^n \big(x_iy_i - x_i\mu(Y) - y_i\mu(X) + \mu(X)\mu(Y)\big)
}{\sqrt{
  \sum\limits_{i=1}^n \big(x_i^2 - 2x_i\mu(X) + \mu^2(X)\big)
  \cdot \sum\limits_{i=1}^n \big(y_i^2 - 2y_i\mu(Y) + \mu^2(Y)\big)
  }
}
\\ &= \frac{
  \sum\limits_{i=1}^n x_iy_i 
  - \mu(Y) \sum\limits_{i=1}^n x_i
  - \mu(X) \sum\limits_{i=1}^n y_i 
  + n\mu(X)\mu(Y)
}{\sqrt{
    \big(
      \sum\limits_{i=1}^n x_i^2 
      - 2\mu(X)\sum\limits_{i=1}^n x_i 
      + n\mu^2(X) 
    \big) \cdot \big(
      \sum\limits_{i=1}^n y_i^2 
      - 2\mu(Y)\sum\limits_{i=1}^n y_i
      + n\mu^2(Y)
    \big)
  }
}
\\ &= \frac{
  \sum\limits_{i=1}^n x_iy_i 
  - n\mu(X)\mu(Y)
}{\sqrt{
    \big(
      \sum\limits_{i=1}^n x_i^2 
      - n\mu^2(X) 
    \big) 
    \cdot \big(
      \sum\limits_{i=1}^n y_i^2 
      - n\mu^2(Y) 
    \big)
  }
}
\\ &= \frac{
  - \frac{1}{2}\sum\limits_{i=1}^n (x_i - y_i)^2
  + \frac{1}{2}\sum\limits_{i=1}^n (x_i^2 + y_i^2)
  - n\mu(X)\mu(Y)
}{
  \sum\limits_{i=1}^n x_i^2 
  - n\mu^2(X)
}
\\ &= \frac{
  - \frac{1}{2}\sum\limits_{i=1}^n (x_i - y_i)^2
  + \sum\limits_{i=1}^n x_i^2
  - n\mu^2(X)
}{
  \sum\limits_{i=1}^n x_i^2 
  - n\mu^2(X)
}
\\ &=
1 - \frac{1}{2}\cdot\frac{
  \sum\limits_{i=1}^nd_i^2
  }{
  \frac{n(n+1)(2n+1)}{6}
  - n\big(\frac{n+1}{2}\big)^2
}
\\ &=
1 - \frac{6}{n(n^2 - 1)}\sum\limits_{i=1}^n d_i^2 
\end{aligned}
$$

---

피어슨 상관계수의 special한 경우이기 때문에 스피어만 상관계수 또한 $-1$에서 $1$사이의 값을 가진다. 여기서 스피어만 상관계수가 $-1$ 또는 $1$이 성립하는 경우를 알아보자. 참고로 스피어만 상관계수가 반드시 $0$이 나오는 일반적인 경우는 없다.

스피어만 상관계수는 피어슨 상관계수처럼 $X$와 $Y$가 선형관계일 때 $1$의 값을 가질 것이다. 하지만 스피어만 상관계수를 계산하기 위한 변수 $X$와 $Y$는 rank이기 때문에, 선형인 경우는 오직 $Y = X$일 때 나타난다. 즉, $Y = X$ if and only if $\rho_s(X, Y) = 1$이다.

다음으로 스피어만 상관계수가 $-1$일 때를 생각해보자. 관측치의 개수 $n$은 상수이므로 $\sum\limits_{i=1}^n d^2_i$가 최대가 되는 두 ranking $X$와 $Y$만 찾으면 된다. 참고로 $d_i$는 $X$와 $Y$의 $i$번째 관측치의 차이이다. 

---

**_proof_**

We may assume that $X = (1, 2, \ldots, n)$. Then we will show that $Y_0 = \max\limits_{Y} \sum\limits_{k=1}^n d_k^2$ if and only if $Y_0 = (n, n-1, \ldots, 1)$.

Our claim will be very similar with the first claim about inner product. Suppose $Y_0 \neq (n, n-1, \ldots, 1)$. 
For any two rank $i < n$ and $j > 1$, consider two equations

$$
(n-1)^2 + (i-j)^2
\\ (n-i)^2 + (j-1)^2.
$$

By following,

$$
\begin{aligned}
\big((n-1)^2 + (i-j)^2\big) &- \big((n-j)^2 + (i-1)^2\big) 
\\ &= (n^2 -2n +1 + i^2 - 2ij + j^2) - (n^2 -2nj + j^2 + i^2-2i+1)
\\ &= (-2n -2ij) - (-2nj-2i)
\\ &= 2n(j-1) -2i(j-1)
\\ &= 2(n-i)(j-1) > 0
\end{aligned}
$$

we obtain $(n-1)^2 + (i-j)^2 > (n-i)^2 + (j-1)^2$. Since $i$ and $j$ are arbitrary, The last observation of $Y_0$ should be $1$. Recursively(or use the hypothesis of mathematical induction), the remain case of $X'=(1, 2, \ldots, n-1)$ and $Y'=(n, n-1, \ldots, 2)$ is the same problem. So, our first claim is complete!

Let us see the other part. If $Y_0 = (n, n-1, \ldots, 1)$, then 

$$
\begin{aligned}
\sum\limits_{k=1}^n d_k^2 &= \sum\limits_{k=1}^n (n-2k+1)^2
\\ &= \sum\limits_{k=1}^n \big(n^2 -2n(2k-1) + (2k-1)^2 \big)
\\ &= n^3 -2n\sum\limits_{k=1}^n (2k-1) + \sum\limits_{k=1}^n(4k^2 - 4k + 1)
\\ &= n^3 - 2n\cdot \big(n(n+1) - n\big) + \frac{4n(n+1)(2n+1)}{6}-\frac{4n(n+1)}{4} + n
\\ &= \frac{n(n^2-1)}{3}.
\end{aligned}
$$

Hence

$$
\begin{aligned}
r_s &= 1 - \frac{6}{n(n^2-1)}\sum\limits_{i=1}^n d_i^2
\\ &= 1 - \frac{6}{n(n^2-1)} \cdot \frac{n(n^2-1)}{3}
\\ &= 1 - 2
\\ &= -1
\end{aligned}
$$

and we are done!