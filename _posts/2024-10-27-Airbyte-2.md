---
title:        Airbyte (2) - 근데 진짜 Ingestion 되는거 맞아요?
date:         2024-10-27
categories:   [Data, Engineering]
comments:     true
---

<style>
H2 { color: #1e7ed2 }
H3 { color: #298294 }
H4 { color: #C7A579 }
</style>

[이전 글](https://ivoryrabbit.github.io/posts/Airbyte-1/){: target="_blank"}에서는 Airbyte가 무엇이고, 어떤 목적으로 사용되는지에 대해 다루었다. Airbyte를 활용하면 다양한 소스로부터 데이터를 수집하고 최소한의 변환만을 수행한 뒤 목적지로 데이터를 전송하는 작업을 간소화할 수 있다.

이번 글에서는 Airbyte가 내부적으로 어떻게 동작하는지 파악하기 위해 백엔드 아키텍처 구성을 살펴보고자 한다. 또한 Helm을 이용해 Kubernetes 환경에서 Airbyte를 배포해보고, Connection을 생성하여 Data Ingestion 작업을 수행하는 실습을 진행할 예정이다.

## Airbyte Architecture

Airbyte의 주요 컴포넌트는 다음과 같다.

#### Web App/UI [airbyte-webapp]

Airbyte의 Web UI를 담당한다. 사용자들은 이곳에서 GUI를 이용해 Airbyte의 전반적인 기능들을 조작할 수 있다.

![image_01](/assets/img/posts/2024-10-27/image_01.png)

#### Config API Server [airbyte-server]

Airbyte의 control plane으로써, `Config`와 `Schedule` 관리를 책임진다.

1. 사용자는 UI(혹은 API)를 통해 서버에게 Source, Destination 및 Connection 생성을 요청할 수 있으며, Config API Server는 이 요청을 토대로 Connection Config, 인증 정보, 스키마를 데이터베이스에 저장한다.

2. 사용자가 새로운 Connection을 생성하면 Config API Server는 후술할 Temporal에게 Workflow를 생성하도록 요청한다. 만약 사용자가 Schedule Type(Scheduled/Cron)을 설정했다면, Config API Server는 Scheduler가 이 정보에 접근할 수 있도록 데이터베이스에 저장한다.

#### Database Config & Jobs [airbyte-db]

Airbyte의 백엔드 데이터베이스이다. 이곳에는 Connection을 생성할 때 사용했던 Config 및 Schedule 데이터가 저장된다. 또한 파이프라인이 실행될 때마다 Sheduler가 성공 여부, 실행 시간 등의 메타데이터를 수집하여 이곳에 저장한다.

#### Temporal Service [airbyte-temporal]

Temporal은 사실 Airbyte의 고유 컴포넌트가 아니라, 비동기 및 분산 처리 작업을 위한 오픈소스 Workflow 엔진이다.

- [Temporal Github](https://github.com/temporalio/temporal){: target="_blank"}

링크된 Github의 README를 보면 durable, resilient 등의 단어들이 등장하는데, 이처럼 Temporal는 작업이 실패하더라도 자동으로 복구 가능한(resilient) 처리 방식을 지향한다.

Temporal Service 속에는 Job의 비동기 처리를 위한 Queue, Job 실행을 위한 Worker, Workflow 정보를 저장하기 위한 데이터베이스가 독립적으로 포함되어 있다.

Workflow는 하나 이상의 Job으로 구성되며 Worker들이 Queue로부터 Job을 하나씩 가져와 처리한다. Job은 독립적인 실행 단위로, 실패하게 되면 Job 단위로 재시도된다. 이 구조는 우리가 익히 알고 있는 Apache Airflow와 구조가 비슷하다.

| Airflow | Temporal |
| --- | --- |
| DAG | Workflow |
| Task | Job |
| 순차 실행에 중점 | 비동기 실행에 중점 |

Airbyte의 Temporal Service는 Connection 마다 Workflow를 생성하고 Scheduler가 예약된 시간에 Workflow을 실행한다. 각 Worker가 Job을 실행함으로써 ETL 작업(=Task)을 트리거하게 되고 Worker는 작업의 성공 여부를 모니터링하고 재시도한다.

#### Worker [airbyte-worker]

Worker는 실제로 Source에 접근하여 데이터를 읽고 Destination에 데이터를 쓰는 역할을 담당한다. 이 Worker은 앞서 언급한 Temporal의 Worker와는 별개의 컴포넌트이다. ~~아~~

앞서 언급한 것과 같이 Temporal 속 Worker가 Job을 실행하면, Job을 이루는 하나 이상의 Task가 Airbyte의 Queue에 추가된다. Airbyte의 Worker는 이 Task를 받아와 실제 ETL 작업을 수행한다. 그리고 그 실행 결과를 후술할 Workload API Server로 전송한다.

#### Workload API Server [airbyte-workload-api-server]

Airbyte에서 Workload API Server는 ETL과 관련된 전반적인 프로세스를 모니터링하고 관리하는 역할을 한다.

ETL 작업을 수행하기 위해 Workload API Server는 Workload Launcher에 실행 요청을 보낸다. 이때 작업에 필요한 Connection Config 정보는 Config API Server에게 요청한다.

Airbyte의 Worker가 Task를 실행한 결과를 모니터링하여, 모두 성공하면 Temporal에게 Job의 완료를 전달한다. 반대로 하나라도 실패한다면 Temporal이 Job을 재시도하여 모든 Task를 다시 트리거하도록 한다.

ETL 작업이 완료되면 Config API Server를 통해 사용자가 작업 결과를 볼 수 있도록 업데이트 한다.

#### Minio [airbyte-minio]

Logs storage를 담당한다. Helm은 기본값으로 minio를 사용하지만 AWS S3 와 같이 external standalone storage를 사용하는 것을 권장한다.

#### 기타 컴포넌트들

- Bootloader [airbyte-bootloader]
- Launcher [airbyte-workload-launcher]
- Cron [airbyte-cron]
    - 오래된 서버 로그와 ETL 작업 결과 로그를주기적으로 청소한다.
    - Connector들을 주기적으로 업데이트한다.
- Pod sweeper [airbyte-pod-sweeper]
    - Kubernetes 사용시 작업 완료한 Pod들을 청소한다.

## Airbyte on K8S

> 다음 Github 링크에 설정들을 상세하게 정리해 두었습니다.
> - [https://github.com/ivoryRabbit/play-data-with-docker/tree/master/airbyte](https://github.com/ivoryRabbit/play-data-with-docker/tree/master/airbyte){: target="_blank"}
{: .prompt-tip }

Airbyte는 백엔드로 마이크로 서비스 아키텍처(MSE)를 채택하고 있다. 따라서 Helm Chart를 통해 Kubernetes 환경에서 Airbyte를 배포해보려 한다. 

### Kubernetes Setting

우선 Kubernetes 환경을 세팅해보자. Docker Desktop을 사용하고 있다면 Settings에서 Kubernetes를 활성화 시킬 수 있다.

![image_02](/assets/img/posts/2024-10-27/image_02.png)

이때 Kubernetes를 조작하기 위해서는 Kubectl이라는 command-line tool이 필요하다. Kubectl은 Brew를 이용해 빠르게 설치할 수 있다.

```shell
brew install kubectl
```

Airbyte를 배포하기 위해 YAML 파일을 일일이 작성하는 것도 좋겠지만 Helm이라는 패키지 관리 도구를 이용하면 좀 더 간편하게 배포할 수 있다. Helm 또한 Brew를 이용해 쉽게 설치 가능하다.

```shell
brew install helm
```

### Airbyte

Airbyte의 경우 이미 Helm Chart가 작성되어 있다. 이를 Helm Client에 다운로드 해주자.

```shell
helm repo add airbyte https://airbytehq.github.io/helm-charts
```

이번 배포 작업을 위해 `airbyte`라는 namespace를 별도로 만들어 준다. 해당 namespace에서 Airbyte를 배포해보자.

```shell
kubectl create namespace airbyte

helm --namespace airbyte install airbyte-local airbyte/airbyte
```

Airbyte를 배포하고 Kubernetes Service 목록을 조회하면 위에서 설명했던 Airbyte 컴포넌트들을 확인할 수 있다.

![image_03](/assets/img/posts/2024-10-27/image_03.png)

### Nginx

Kubernetes에 Airbyte를 배포했다면 이제 local에서 UI에 접근할 수 있도록 포트 포워딩해주어야 한다. 이때 nginx ingress controller를 사용하면 간편하게 작업할 수 있다.

```shell
helm --namespace airbyte install airbyte-nginx oci://ghcr.io/nginxinc/charts/nginx-ingress --version 1.4.0
```

#### Port Forwarding

Local host로 접근 시 airbyte-webapp 서비스를 라우팅할 수 있도록 YAML 파일을 작성하고 Service에 등록해주자. 그러면 [http://localhost](http://localhost)를 통해 UI에 접근할 수 있다.

##### [ingress/nginx.yaml]
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: airbyte-ingress
  annotations:
    ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:
    - host: localhost
      http:
        paths:
          - backend:
              service:
                name: airbyte-local-airbyte-webapp-svc
                port:
                  number: 80
            path: /
            pathType: Prefix

```

```shell
kubectl --namespace airbyte apply -f ingress/nginx.yaml
```

#### Docker

이제 Connection을 한번 생성해보자. Source로는 MySQL를, Destination은 MinIO를 사용하여 배포하려 한다.

아래와 같이 Docker compose 용 YAML 파일을 작성하여 배포해보자.

```yaml
services:
  mysql:
    container_name: mysql
    hostname: mysql
    image: mysql:8.0
    ports:
      - "3306:3306"
    command:
      - --character-set-server=utf8mb4
      - --collation-server=utf8mb4_unicode_ci
    volumes:
      - ./docker/volume/mysql:/var/lib/mysql
      - ./docker/mysql/:/docker-entrypoint-initdb.d/
    environment:
      TZ: Asia/Seoul
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-test}
      MYSQL_USER: ${MYSQL_USER:-airbyte}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-airbyte}
      MYSQL_DATABASE: ${MYSQL_DATABASE:-dev}
      MYSQL_ALLOW_EMPTY_PASSWORD: true

  minio:
    container_name: minio
    hostname: minio
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
      MINIO_DOMAIN: minio
    command: server /data --console-address ":9001"
    volumes:
      - ./docker/volume/minio:/data
    healthcheck:
      test: "mc ready local"
      interval: 10s
      retries: 3
      start_period: 5s

  minio-client:
    container_name: minio-client
    hostname: minio-client
    image: minio/mc
    entrypoint: >
      /bin/bash -c "
      mc config --quiet host add storage http://minio:9000 minio minio123 || true;
      mc mb --quiet --ignore-existing storage/hive || true;
      "
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_REGION: ap-northeast-2
      AWS_DEFAULT_REGION: ap-northeast-2
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: true
    depends_on:
      minio:
        condition: service_healthy
```

MySQL에서는 실습에 사용할 `category` 테이블을 생성하기 위해 *docker-entrypoint-initdb.d* 경로에 다음 파일을 추가해주었다.

##### [init.sql]
```sql
USE dev;

CREATE TABLE IF NOT EXISTS category (
    id INT NOT NULL AUTO_INCREMENT,
    category_name VARCHAR(20) NOT NULL,
    PRIMARY KEY (id)
);

INSERT INTO category (category_name) VALUES 
    ('apparel'),
    ('food'),
    ('furniture'),
    ('grocery'),
    ('etc')
;
```

MinIO의 경우 원하는 AWS_REGION, S3_ENDPONT에 *hive* 버킷을 생성하기 위해 *minio-client* 서비스를 docker compose를 통해 실행해주었다.

#### Connection

##### [Source]

이제 Source를 등록해보자. Kubernetes의 컨테이너에서 Localhost로 접근하는 케이스이므로, Host에는 `host.docker.internal`를 입력해준다.

![image_04](/assets/img/posts/2024-10-27/image_04.png)

또는 별도의 External Name Service를 배포하여 Alias를 사용할 수 있다.

```yaml
kind: Service
apiVersion: v1
metadata:
  name: airbyte-external
spec:
  type: ExternalName
  externalName: host.docker.internal
```

##### [Destination]

MinIO를 Destination으로 등록하기 위해서는 S3 Connector를 사용해야 한다. 이때 S3 Access Key 정보는 MinIO Consol(localhost:9001)에서 Access Key를 생성하여 사용할 수 있다.

Source 때와 마찬가지로 Optional fields의 S3 Endpoint 항목에 `http://host.docker.internal`를 채워넣어주자.

![image_05](/assets/img/posts/2024-10-27/image_05.png)

#### Result

Source와 Destination 등록 후 Connection을 생성할 수 있다. 이때 Sync할 테이블 목록을 확인할 수 있으며, Schedule Type 등을 세팅할 수 있다.

![image_06](/assets/img/posts/2024-10-27/image_06.png)

Schedule Type을 Manual로 선택한 후 Connection을 생성하고 상단의 `Sync Now` 버튼을 클릭해주자.

그러면 MySQL의 *category* 테이블이 MinIO의 *hive* 버킷 속으로 Sync된 것을 확인할 수 있다.

![image_07](/assets/img/posts/2024-10-27/image_07.png)