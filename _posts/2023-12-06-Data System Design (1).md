---
title:        데이터 시스템 디자인 (1)
date:         2023-12-09
categories:   [Data, Engineering]
comments:     true
---

<style>
H2 { color: #cd853f }
H3 { color: #deb887 }
</style>

## 들어가기 앞서

데이터 엔지니어가 되고 지금까지 3년이 약간 모자란 정도의 시간이 흘렀습니다.

신입 시절을 돌아보면 다음과 같은 자잘한 의문들을 많이 가졌던 것 같습니다.

_<p style="text-align: center;">"왜 귀찮게 매일 snapshot을 찍는 걸까?"</p>_
_<p style="text-align: center;">"왜 하필 새벽에 스케쥴을 거는 거지?"</p>_

3년이나 지났으면 이제 그려러니 하고 넘어갈 법한데 저는 하남자이기 때문에 이런 의문에 집착하지 않을 수 없었습니다.

앞으로는 현업에서 보고 느낀 이러한 의문들을 기록하고, 이를 토대로 데이터 엔지니어에게 도움이 될 만한 내용들을 정리해보려고 합니다.

### 데이터 엔지니어란?

데이터 엔지니어의 역할은 사내 구성원들이 데이터를 잘 활용할 수 있도록 안정적이고 값싼(?) 분석 환경을 제공하는 것입니다.

예를 들어 데이터 처리 파이프라인을 담당하는 데이터 엔지니어는 데이터 처리 어플리케이션이 실패하더라도 재실행을 통해 데이터가 복구될 수 있도록 안정적인 어플리케이션을 개발해야 합니다.

또한 데이터 인프라를 담당하는 데이터 엔지니어는 회사의 데이터 규모와 분석 소요를 고려하여 효율적인 분석 환경을 도입 및 운영해야 합니다.

데이터 엔지니어에게 요구되는 역할은 회사마다, 팀마다 조금씩 차이가 있겠지만 본질적으로는 데이터를 수집 및 가공하는 기술을 바탕으로 업무를 수행하게 됩니다.

이번 글에서는 데이터 엔지니어가 겪는 모든 비극의 시작, Data Ingestion에 대해 다루어 보려고 합니다. 그전에 간단한 가정과 용어 정리를 하고 넘어가겠습니다.

### 가정

#### 1. 서비스 환경과 분석 환경은 분리되어 있습니다.

서비스를 사용하는 고객들이 불편을 겪지 않도록 분석 환경을 서비스 환경으로부터 분리하고, 데이터를 서비스 환경에서 분석 환경으로 흘려보내는 것이 바로 데이터 엔지니어가 존재하는 이유입니다.

#### 2. 분석 환경은 하나로 통일되어 있습니다.

데이터를 충분히 잘 분석하기 위해서는 조인이 필요하며, 이를 위해 하나의 논리적 공간에서 모든 데이터가 조회가능해야 합니다. 이 글에서는 이러한 논리적 공간을 DW(Data Warehouse)라고 부르겠습니다.

## <o>Data Ingestion</o>

Data Ingestion은 데이터를 추출하여 DW로 적재하는데에만 집중하기 위해 기존의 ETL(Extract-Transform-Load) 방식에서 따로 떨어져 나온 개념입니다.

Data Ingestion이 ETL과 분리된 이유는 제 경험 상 다음과 같습니다.

#### 1. 서비스 환경에 직접 접근하기 때문에 부하에 예민합니다. 
분석을 위해 서비스 이용자들이 불편을 겪게 해선 안되니까요.

#### 2. 데이터 추출이 필요한 source가 너무나도 다양합니다. 
반면 데이터 엔지니어는 어떤 source를 사용할 것인지 결정할 수 없습니다.

#### 3. 보안에 신경써야 합니다.
데이터가 분석 환경으로 옮겨지면 임직원들에게 노출되므로 민감한 정보는 이 단계에서 걸러져야 합니다.

이러한 Data Ingestion 파이프라인을 구축할 때는 데이터가 어떻게 생겨 먹었는지 알아야 합니다.

### Mutable vs Immutable
업데이트가 발생하는 데이터인지 아닌지 고려해야 합니다.

상품 정보 데이터는 상품 이름이나 카테고리가 언제든 변경될 수 있습니다.
반면 상품 클릭 로그는 한번 쌓이면 그 값이 변경되지 않습니다.

### Fact Table vs Dimension Table
Dimension Table은 관측하려는 객체를 기준으로 메타 데이터를 기록한 테이블입니다.

예를 들어 상품 정보 데이터는 상품 ID를 primary key로 하여 하나의 row가 그 상품의 정보를 나타냅니다.

반면 Fact Table은 두 개 이상의 객체를 매핑한 테이블입니다.

예를 들어 상품 구매 데이터는 유저가 상품을 구매한 이벤트가 하나의 row로 기록됩니다.

둘다 mutable 일수도, immutable일 수도 있습니다.

예를 들면 
상품 정보 -> Mutable, Dimension Table


### Batch vs Streaming
데이터를 옮겨담는 방식

이번 글에서는 위의 요소들을 고려하여 Target Table을 어떻게 디자인하면 좋을지에 대해 다루어보려 합니다.

### 1. Snapshot
RDB에 적재된 테이블을 통째로 snapshot 찍어 업로드

transaction data로, 테이블에서 업데이트가 일어나는 경우

dimension table 이거나
데이터 규모가 작거나
정산과 같이 분석 단에서 예전 데이터를 봐야만 하는 경우


### 2. Incremental
적재된 테이블이 insert만 발생하는 경우
로그
적재할 테이블을 partition으로 운영하되, 증분으로 데이터를 집어 넣어준다

데이터가 생성된 날짜를 가리키는 flag 필요
- source가 rdb라면 row가 생성된 created_at
- source가 stream 이라면 스토리지에 적재된 시간 필요

### 3. Delta
기존 테이블을 계속 유지하면서 source table의 변경 분만 가져다가 target table에 merge하는 방식

target은 source와 계속 sync가 맞춰짐

### 4. Delta + Incremental
테이블을 Delta로 운영하여 target table의 최신 상태를 유지하되, 중요한 event들은 변경이 일어날때마다 별도의 테이블에 incremental하게 업데이트

### 5. Snapshot + Delta
Snapshot을 찍되, Snapshot을 찍은 시점 이후의 Delta를 모아 다시 Snapshot을 생성하는 방식
source와 target이 비교적 낮은 latency로 sync될 필요가 있는 경우

### 6. Stream
Insert만 발생하는 경우에 사용 가능

쿼리를 날리는 동시에 지금까지 모은 데이터를 consuming하는 방식
쿼리가 들어오지 않으면 지정해둔 파일 사이즈가 될 때까지 파일을 모았다가 자동으로 consume

### 7. Stream + Delta (CDC)
데이터가 생성될 때 마다 원본 테이블에 Delta 데이터를 upsert
다만 추후 읽기 성능을 위해서 주기적으로 파일을 compact하게 정리해주어야 함

Apache Pinot

