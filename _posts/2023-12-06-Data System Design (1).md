---
title:        데이터 시스템 디자인 (1)
date:         2023-12-09
categories:   [Data, Engineering]
comments:     true
---

<style>
H2 { color: #d2691e }
H3 { color: #cd853f }
H4 { color: #deb887 }
</style>

## 들어가기 앞서

데이터 엔지니어가 되고 지금까지 3년이 약간 모자란 정도의 시간이 흘렀습니다.

입사 당시를 돌아보면 지금은 당연하게 생각하는 것들에 의문을 품곤 했었습니다.

_"왜 모든 snapshot을 저장하고 있는 걸까?"_

_"왜 하필 새벽에 스케쥴을 거는 거지?"_

한 때 업무를 수행하면서 가졌던 이러한 의문들을 정리하고, 이를 토대로 데이터 엔지니어에게 도움이 될 만한 내용의 글들을 써보려고 합니다.

첫 주제로는 데이터 엔지니어가 겪는 모든 비극의 시작, Data Ingesion에 대해 다루어 보려 합니다. 그전에 간단한 가정과 용어 정리를 하고 넘어가겠습니다.

<!-- ### 데이터 엔지니어란?

데이터 엔지니어의 역할은 사내 구성원들이 데이터를 잘 활용할 수 있도록 안정적이고 값싼(?) 분석 환경을 제공하는 것입니다.

데이터 엔지니어에게 요구되는 역할은 회사마다, 팀마다 조금씩 차이가 있겠지만 본질적으로는 데이터를 수집 및 가공하는 기술을 바탕으로 업무를 수행하게 됩니다.

예를 들어 데이터 처리 파이프라인을 담당하는 데이터 엔지니어는 데이터 처리에 실패하더라도 재실행을 통해 데이터가 복구될 수 있도록 어플리케이션을 개발해야 합니다.

또한 데이터 인프라를 담당하는 데이터 엔지니어는 회사의 데이터 규모와 분석 소요를 고려하여 효율적인 분석 환경을 도입 및 운영해야 합니다. -->

### 가정

#### 1. 서비스 환경과 분석 환경은 분리되어 있습니다.
서비스를 사용하는 고객들이 불편을 겪지 않도록 분석 환경을 서비스 환경으로부터 분리하고, 데이터를 서비스 환경에서 분석 환경으로 흘려보내는 것이 바로 데이터 엔지니어가 존재하는 이유입니다.

#### 2. 분석 환경은 하나로 통일되어 있습니다.
데이터를 충분히 잘 분석하기 위해서는 조인이 필요하며, 이를 위해 하나의 논리적 공간에서 모든 데이터가 조회가능해야 합니다. 이 글에서는 이러한 논리적 공간을 DW(Data Warehouse)라고 부르겠습니다.

## Data Ingestion

Data Ingestion은 데이터를 추출하여 DW로 적재하는데에만 집중하기 위해 기존의 ETL(Extract-Transform-Load) 방식에서 따로 떨어져 나온 개념입니다.

Data Ingestion이 유독 중요해진 이유는 다음과 같은 문제 때문이라 생각됩니다.

#### 1. 서비스 환경에 직접 접근하기 때문에 부하에 예민합니다. 
분석을 위해 서비스 이용자들이 불편을 겪게 해선 안되니까요.

#### 2. 데이터 추출이 필요한 source가 너무나도 다양합니다. 
반면 데이터 엔지니어는 어떤 source를 사용할 것인지 결정할 수 없습니다.

#### 3. 보안에 신경써야 합니다.
데이터가 분석 환경으로 옮겨지면 임직원들에게 노출되므로 민감한 정보는 이 단계에서 걸러져야 합니다.

데이터를 가공 또는 변환(Transform)하는 동시에 위의 문제들을 해결하기는 생각보다 어렵습니다. 따라서 일단 추출(Extract)한 그대로의 데이터를 DW에 적재(Load)하는 방식을 주로 이용합니다.

이때 적재하고자 하는 데이터가 어떠한 특징을 가지냐에 따라 Data Ingestion의 전략을 다르게 가져갈 수 있습니다.

#### Mutable Data and Immutable Data
데이터가 이미 한번 기록된 후에도 업데이트가 발생한다면 Mutable Data 라고 합니다.

예를 들어 상품 정보 데이터는 상품 이름이나 가격이 얼마든지 변경될 수 있습니다.

반면 데이터가 이미 생성되어 수정이 불가능한 경우에는 Immutable Data 라고 합니다.

상품 클릭 로그는 한번 쌓이면 그 값이 다시 변경되지 않습니다.

<!-- #### 2. Fact Table and Dimension Table(WIP)
** 좀 더 좋은 표현을 찾아보고 있습니다.

Fact Table은 비지니스에서 정의된 이벤트(구매, 클릭, 로그인 등)를 하나 이상의 객체의 조합으로 기록하는 테이블입니다. Transaction 또는 Log가 이에 속합니다.

예를 들어 상품 구매 정보 테이블은, 비지니스에서 정의된 "구매"라는 사실을 (고객, 상품, 날짜) 조합의 단위로 고유하게 기록하게 됩니다.

반면 Dimension Table은 비지니스에서 정의된 객체(고객, 상품 등)의 고유 속성을 기록하고 있는 테이블입니다.

예를 들어 상품 정보 테이블은 상품 ID라는 primary key를 기준으로 다른 모든 속성들이 그 상품의 특징을 나타내도록 기록하게 됩니다. -->

<!-- 상품 구매 데이터는 고객이 상품을 구매한 이벤트가 하나의 row로 기록되는 Fact Table입니다. 이때 고객 ID와 상품 ID는 각 객체의 정보를 담은 Diemsion Table를 가리키는 foreign key가 됩니다.

로그인 데이터는 고객이 웹 페이지에 로그인한 이벤트가 하나의 row로 기록되는 Fact Table입니다. -->

<!-- - 상품 정보 -> Mutable Data, Dimension Table
- 상품 상태 코드 정보 -> Immutable Data, Dimension Table
- 상품 주문 정보 -> Mutable Data, Fact Table
- 상품 클릭 정보 -> Immutable Data, Fact Table -->


<!-- ### Batch vs Streaming
데이터를 옮겨담는 방식

이번 글에서는 위의 요소들을 고려하여 Target Table을 어떻게 디자인하면 좋을지에 대해 다루어보려 합니다. -->

이러한 특징을 고려하여 DW의 테이블을 설계할 때 다양한 방식을 적용할 수 있습니다.

### 1. Snapshot
가장 보편적이고 간편한 적재 방법입니다.

<!-- Mutable Data를 적재하는데 효과적인 방법입니다. -->

날짜 파티션을 만들고 주기적으로 스냅샷을 찍어 각 파티션에 저장 합니다.

#### 장점
- Ingestion 방식이 가장 간단합니다.
- 매일 스냅샷을 찍어두면 분석 단에서 예전 데이터를 신뢰하고 분석할 수 있습니다.

#### 단점
- 모든 데이터를 추출하기 때문에 데이터가 많을 수록 서비스 환경에 큰 부하를 줍니다.
    - 따라서 주로 트래픽이 적은 새벽에 작업이 진행됩니다.
- 데이터의 모든 변경 사항을 나타내진 않습니다.
- 데이터가 많을 수록 저장 비용이 크게 증가합니다.

<!-- transaction data로, 테이블에서 업데이트가 일어나는 경우 -->

<!-- 정산과 같이 분석 단에서 예전 데이터를 봐야만 하는 경우에 사용합니다. -->

### 2. Accumulative
만약 Immutable Data를 적재하는 경우라면 Snapshot을 찍을 필요가 없습니다.

적재할 데이터가 수정되지 않기 때문에 날짜 파티션을 만들고 그 날짜에 생성된 데이터를 각 파티션에 저장할 수 있습니다.

#### 장점
- 날짜 별로 파티션되어 있어 특정 상황에서 데이터 조회가 빠릅니다.
- Snapshot에 비해 저장 비용이 저렴합니다.

#### 단점
- Immutable Data를 적재할 때만 사용 가능한 방법입니다.
- Source 테이블에 created_at 등 생성이 발생한 flag 역할의 컬럼을 추가해주어야 합니다.

### 3. Incremental
Snapshot은 편리하지만 데이터의 크기가 커지면 여러 가지 문제가 발생하게 됩니다.

만약 예전 데이터를 볼 필요가 없다면 주기적으로 증분 데이터만 추출하여 기존 테이블에 반영하는 방법도 좋습니다.

#### 장점
- 증분 데이터만 추출하기 때문에 부하가 적고 처리 속도가 빠릅니다.
- 따라서 Ingestion 주기를 줄일 수 있어 새로운 데이터가 반영되는 latency를 줄일 수 있습니다.
- Snapshot에 비해 저장 비용이 저렴합니다.

#### 단점
- 예전 데이터를 더 이상 신뢰할 수 없습니다.
- Source 테이블에 modified_at 등 변경이 발생한 flag 역할의 컬럼을 추가해주어야 합니다.

### 4. Snapshot + Delta
변경이 자주 일어나지 않는 테이블의 경우 Snapshot을 찍는 것이 매우 비효율적일 수 있습니다.

이런 경우에는 변경이 일어난 데이터만 추출하여 마지막 파티션과 merge하는 식으로 새로운 파티션을 생성할 수 있습니다.

#### 장점
- 증분 데이터만 추출하기 때문에 서비스 환경에 주는 부하가 적습니다.

#### 단점
- 테이블의 크기가 크다면 merge가 단순 insert보다 처리 시간이 더 오래 걸릴 수 있습니다.
- Source 테이블에 modified_at 등 변경이 발생한 flag 역할의 컬럼을 추가해주어야 합니다.
- Snapshot과 동일한 저장 비용이 발생합니다.

### 5. Capture Data Change
데이터가 변경되는 정보가 분석에 필요한 경우도 있습니다.

이러한 경우에는 테이블을 최신의 상태로 동기화만 시켜주되 변경분은 별도의 테이블에 저장하여 분석에 사용할 수 있습니다.

#### 장점
- 분석단에서 데이터가 변경된 모든 내역을 사용할 수 있습니다.
- 필요하다면 변경분과 조인하여 해당 날짜에 대한 스냅샷을 생성할 수 있습니다.

#### 단점
- 데이터 변경 로그를 저장하는 별도의 파이프라인을 개발해야 합니다. 


<!-- ### 3. Delta -->
<!-- Mutable Data + Fact Table을 적재하는데 효과적인 방법입니다. -->

<!-- 기존 테이블을 지속적으로 유지하면서 source의 변경 분만 가져다가 target에 반영하는 방식입니다. -->

<!-- ### 4. Delta + Incremental
테이블을 Delta로 운영하여 target table의 최신 상태를 유지하되, 중요한 event들은 변경이 일어날때마다 별도의 테이블에 incremental하게 업데이트

### 5. Snapshot + Delta
Snapshot을 찍되, Snapshot을 찍은 시점 이후의 Delta를 모아 다시 Snapshot을 생성하는 방식
source와 target이 비교적 낮은 latency로 sync될 필요가 있는 경우

### 6. Stream
Insert만 발생하는 경우에 사용 가능

쿼리를 날리는 동시에 지금까지 모은 데이터를 consuming하는 방식
쿼리가 들어오지 않으면 지정해둔 파일 사이즈가 될 때까지 파일을 모았다가 자동으로 consume

### 7. Stream + Delta (CDC)
데이터가 생성될 때 마다 원본 테이블에 Delta 데이터를 upsert
다만 추후 읽기 성능을 위해서 주기적으로 파일을 compact하게 정리해주어야 함

Apache Pinot -->

