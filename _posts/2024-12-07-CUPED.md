---
title:        CUPED - A/B 테스트를 빨리 끝내고 싶다면
date:         2024-12-07
categories:   [Data, Analysis]
math:         true
comments:     true
---

<style>
H2 { color: #298294 }
H3 { color: #1e7ed2 }
H4 { color: #C7A579 }
</style>

---

## A/B 테스트란?

**A/B 테스트**란 사용자 경험 개선을 위해 두 가지의 다른 옵션(A와 B)을 비교하여 최적의 선택을 도출하는 실험 설계 기법이다. 데이터 분석 환경이 갖추어진 많은 회사들이 UI 혹은 알고리즘을 개선할 때 A/B 테스트를 이용해 의사결정하고 있다.

통계학에서는 **가설 검정**이란 이름으로 더 잘 알려져 있는데, 온라인 실험 환경에서 데이터 기반 의사결정을 목적으로 수행되는 경우에 특별히 A/B 테스트라고 부르는 듯 하다.

이해를 돕기 위해 웹 사이트에서 버튼 색상을 변경했을 때 클릭률(Click-Through Rate, CTR)에 어떤 영향을 미치는지 분석하는 상황을 가정해보자.

| 유형 | 설명 | 표본 수 | CTR |
| :--- | :--- | ---: | ---: |
| 대조군(A) | 흰색 배경에 회색 버튼 | 100 | 13% |
| 실험군(B) | 흰색 배경에 빨간색 버튼 | 100 | 15% |

빨간색 버튼의 CTR이 회색 버튼보다 높으므로 우리는 실험 결과를 토대로 버튼 색깔을 빨간색으로 교체하도록 결정을 내릴 수 있다.

---

### p-value

하지만 이렇게 결론 내려도 문제가 없는걸까? 다시 의사결정 과정으로 되돌아가보자.

기존에는 회색 버튼을 사용하고 있었기 때문에 최악의 상황은 바로 <u>버튼 색깔을 바꾸었음에도 불구하고 CTR이 감소</u>하는 것이다. 따라서 우리는 긍정이라 판단했지만 실제론 아닌 경우(False Positive, FP)에 유의하여 의사결정을 해야 했다.

| 결정\결과 | CTR 상승 | CTR 유지 | CTR 감소 |
| --- | :--- | :--- | :--- |
| 색깔 유지(N) | - | 당연함(TN) | - |
| 색깔 교체(P) | 뿌듯함(TP) | 아쉬움(FP) | 최악(FP) |

다행히도 고등학교에서 이를 1종 오류라고 정의하고 정량적으로 측정할 수 있는 수단을 배웠~~었~~는데, 바로 **p-value**이다. p-value는 실험군의 데이터가 대조군의 분포로부터 관측될 확률을 의미한다.

p-value가 높을수록 실험군의 통계값을 신뢰하기 어렵기 때문에, 실험 전에 미리 정해둔 **유의수준**을 넘겼다면 실험 결과를 의사결정에 사용하는데 주의를 기울여야 한다.

Python을 이용해 예시에서의 p-value를 한번 계산해보자.

```python
from scipy.stats import binomtest

test = binomtest(k=15, n=100, p=0.13, alternative="greater")
print(f"p-value: {test.pvalue:0.3f}")
```

#### [실행 결과]
```
p-value: 0.317
```

보통 유의수준을 0.05 ~ 0.10 사이로 설정한다는 점을 고려해보면, 0.317이라는 수치는 너무 높다. 따라서 해당 실험의 결과를 신뢰하기는 아직 어려울 것이다.

---

### z-test

앞선 실험에서는 클릭률을 살펴봤었지만, A/B 테스트에서 표본 집단의 확률 분포가 꼭 Binomial 일리란 법이 없다. 그러나 중심 극한 정리(Central Limit Theorem, CLT) 덕분에 표본 수가 적당히 클 때($n \geq 30$) 표본 평균의 확률 분포를 정규 분포로 가정할 수 있다.

$$ \bar{X} \sim N(\mu, \frac{\sigma^2}{n}) $$

즉, 모집단의 평균과 분산이 각각 $\mu$, $\sigma$이고 표본 개수가 $n$일 때 표본 평균의 통계량은 다음과 같이 정의되며

$$\bar{X} = \frac{\sum X_i}{n}$$

$\bar{X}$의 평균과 분산은 다음과 같다.

$$\mu_{\bar{X}} = \mu$$

$$\sigma_{\bar{X}}^2=\frac{\sigma^2}{n}$$

이 가정 하에 z-test를 수행하면 다음과 같은 결과를 얻는다.

$$ Z = \frac{\bar{X} - \bar{\mu}}{\bar{\sigma}}$$

```python
from scipy.stats import norm

n = 100
mean = 0.13
std = (1/n * 0.13 * (1-0.13)) ** 0.5
z_score = (0.15 - mean) / std

p_value = 1 - norm.cdf(z_score)
print(f"p-value: {p_value:0.3f}")
```

#### [실행 결과]
```
p-value: 0.276
```

---

### t-test

사실 A/B 테스트에서 모집단의 확률 분포를 모르는 경우가 많다. 이럴땐 어쩔 수 없이 표본집단으로부터 모집단의 모수를 추정할 수 밖에 없는데, 이를 **추정량**(estimator)이라고 부른다. 그리고 그 기대값이 모집단의 모수와 같은 경우(unbiased esimator)를 주로 사용한다.

표본 평균 $\bar{X}$의 기대값은 다행히 모집단의 평균 $\mu$와 동일하고 표본 분산의 기대값은 모집단 분산의 $\frac{n-1}{n}$ 이므로, 수식을 약간 수정하여 unbiased estimator를 계산해 낼 수 있다.

$$\hat{\mu} = \bar{X}$$

$$\hat{s}^2 = \frac{\sum(X_i - \bar{X})^2}{n-1}$$

모집단의 분산 대신 추정량을 사용하면 더 이상 정규 분포를 가정할 수 없다. 따라서 모집단의 분산을 모르는 경우에는 t 분포를 사용해야 한다.

$$ t = \frac{\bar{X} - \mu_{\bar{X}}}{s_{\bar{X}}} $$

```python
from scipy.stats import t

n = 100
mean_a, std_a = 0.13, (1/(n-1) * 0.13 * (1-0.13)) ** 0.5
mean_b, std_b = 0.15, (1/(n-1) * 0.15 * (1-0.15)) ** 0.5

se = (std_a ** 2 + std_b ** 2) ** 0.5
t_score = (mean_b - mean_a) / se
df = n - 1

pvalue = 1 - t.cdf(t_score, df=df)
print(f"p-value: {pvalue:0.3f}")
```

#### [실행 결과]
```
p-value: 0.343
```

---

### Variance

위의 t-test 수식을 놓고 곰곰이 생각해보면 추정량의 표준편차, 즉 표준오차(Standard Error, SE)가 작아질 수록 p-value가 작아진다. 이는 추정량의 분산(표준오차의 제곱)과 p-value가 비례한다는 의미이기도 하다.

| 표본 수(n) | 표준오차(SE) | t-score | ... | p-value |
| :---: | :---: | :---: | :---: | :---: |
| ↗ | ↘ | ↗ | | ↘ |
| ↘ | ↗ | ↘ | | ↗ |

결국 유의수준 보다 낮은 p-value를 얻기 위해서는 분산이 작아질 때까지 표본 수가 쌓이길 기다려야 한다. 예시를 통해서도 표본 수를 1000개로 늘렸을 때 p-value가 떨어지는 것을 볼 수 있다.

```python
from scipy.stats import t

n = 1000
mean_a, std_a = 0.13, (1/(n-1) * 0.13 * (1-0.13)) ** 0.5
mean_b, std_b = 0.15, (1/(n-1) * 0.15 * (1-0.15)) ** 0.5

se = (std_a ** 2 + std_b ** 2) ** 0.5
t_score = (mean_b - mean_a) / se
df = n - 1

pvalue = 1 - t.cdf(t_score, df=df)
print(f"p-value: {pvalue:0.3f}")
```

#### [실행 결과]
```
p-value: 0.099
```

## CUPED

우리는 위에서 다룬 이론적 배경을 토대로 표본 수가 많아질 수록 p-value가 감소하는 것을 보았다. 하지만 A/B 테스트가 실제로 진행되는 온라인 환경에서는 표본 수가 쌓일 때까지 마냥 기다릴 수만은 없다. 왜냐하면 실험군과 대조군의 차이를 **민감**하게 잡아내지 못하면 다음과 같은 문제들이 발생할 수 있기 때문이다.

#### 1. 부정적인 사용자 경험

만약 신규 기능이 사용자 경험에 부정적인 영향을 끼친다면, A/B 테스트가 길어질 수록 실험군의 이탈이 커질 수 있다.

#### 2. 트래픽의 증가

실험의 개수가 많고 실험 기간이 길수록 실험에 할당되는 트래픽이 커지게 된다. 그러다보면 실험 간에 영향을 줄 수도 있기 때문에 최소한의 트래픽을 유지하는 것이 좋다.

#### 3. 너무 적은 표본

특정 세그먼트에만 영향을 주는 기능이라면 표본의 크기가 작을 수 있다. 이런 경우엔 결과를 얻는데까지 상당한 시간이 걸릴 수 있다.

---

### Improving Sensitivity

그렇다면 A/B 테스트 시 어떻게 이 민감도를 높일 수 있을까? 20213년 Microsoft는 분산을 감소시킴으로써 민감도를 높이는 **CUPED**(Controlled Using Pre-Existing Data) 방식을 소개하였다.

#### [논문 링크]

- [Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data](https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf){: target="_blank"}

논문은 이전까지 민감도를 높이는 연구들이 특정 상황에서만 사용 가능했음을 지적하며 더 범용적인 방법을 제안하였다. 대조군과 실험군에 대해 사전 실험 데이터(Pre-Experiment Data)를 활용하여 실험 결과의 분산을 줄이고 민감도를 높일 수 있다고 주장하고 있다.

- CUPED는 사전 데이터를 사용해 온라인 실험의 분산을 줄이고 민감도를 크게 향상
- 비사용자 기반 지표와 일부 사전 데이터가 누락된 경우에도 적용 가능
- 사전 실험 단계에서 동일 지표를 사용하는 것이 가장 큰 분산 감소를 가져온다는 경험적 결과 소개
- Bing에서 실행된 실제 온라인 실험에서 약 50%의 분산 감소를 보였으며, 이는 트래픽을 두 배로 늘리거나 실험 시간을 절반으로 줄이는 것과 같은 효과
- 최적의 사전 실험 기간 설정과 다중 공변량 사용 등 성공적인 CUPED 적용을 위한 가이드 제공

---

### Methods

CUPED는 추정량의 분산을 줄이기 위해 다음 두 가지 Monte-Carlo sampling을 사용한다.

#### 1. Control Variates Method

사전 데이터(Covariate)를 사용하여 분산을 줄인다. (correlation이 클수록 분산이 더 작아짐)

$$ \hat{Y}_{cv} := \bar{Y} - \theta \bar{X} + \theta \mathbb{E}[X] $$

이때 $\theta$는 상수이고 $\theta \bar{X} + \theta \mathbb{E}[X]$의 기대값은 0이기 때문에 $\hat{Y}$는 여전히 unbiased estimator이다.

이제 $\bar{Y}_{cv}$의 분산을 계산하고

$$ \text{var}(\bar{Y}_{cv}) = \text{var}(\bar{Y} - \theta \bar{X}) = \frac{1}{n}(\text{var}(Y) + \theta^2 \text{var}(X) - 2\theta \text{cov}(Y, X)) $$

$\bar{Y}_{cv}$를 최소로 하는 $\theta$를 대입하면

$$ \theta = \frac{\text{cov}(Y, X)}{\text{var}(X)} $$

우리는 다음과 같은 식을 얻을 수 있다.

$$ \text{var}(\bar{Y}_{cv}) = \frac{\text{var}(Y)}{n} - \frac{\text{var}(Y)}{n} \cdot \frac{\text{cov}(Y, X)^2}{\text{var}(Y) \cdot \text{var}(X)} = \text{var}(\bar{Y})(1 - \text{cor}(Y, X)^2) $$

이때 $\rho = \text{cor}(Y, X)$는 -1과 1 사이의 값을 가지므로

$$ \text{var}(\bar{Y}) \geq \text{var}(\bar{Y})(1 - \rho^2) = \text{var}(\bar{Y}_{cv})$$


#### 2. Stratification Method

표본 집단을 더 잘게 쪼개어 각각의 평균을 추정량으로 선택한다.

$$ \hat{Y}_{strat} := \sum_k w_k \bar{Y}_k $$

$$ \text{var}(\bar{Y}) = \sum_k \frac{w_k}{n}\sigma^2_k + \sum_k \frac{w_k}{n} (\mu_k - \mu)^2 \geq 
\sum_k \frac{w_k}{n}\sigma^2_k = \text{var}(\hat{Y}_{strat}) $$

---

### Practice

실제 A/B 테스트에는 위 두가지 방법을 다음과 같이 적용한다.

#### 1. Control Variates Method

실험 시작 이전의 데이터를 사전 데이터로 채택한다.

#### 2. Stratification Method

실험 시작 이전의 데이터에 포함된 유저 / 포함되지 않은 신규 유저 / 이탈 유저
