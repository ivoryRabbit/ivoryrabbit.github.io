---
title:        Project Nessie로 데이터 버전 관리하기
date:         2024-01-04
categories:   [Data, Engineering]
tags:         [nessie, iceberg, data lake]
comments:     true
image:
    path:     /assets/img/posts/2025-01-04/main.png
    alt:      project nessie
description:  Apache Iceberg를 사용할 때 데이터의 version control을 돕는 Project Nessie를 살펴보겠습니다.
---

<style>
H2 { color: #298294 }
H3 { color: #1e7ed2 }
H4 { color: #C7A579 }
</style>

## Project Nessie

과거에는 분석 환경에서 데이터가 담긴 파일을 분산 처리하기 위해 주로 Apache Hive를 사용했다. 이때 Hive는 테이블 스키마 등의 메타데이터를 저장하고 API를 통해 저장된 메타데이터 정보를 전달하는 수단, 즉 **카탈로그** 서비스를 위해 Hive metastore를 사용한다.

Project Nessie는 이 Hive metastore를 대체할 수 있는 카탈로그 오픈 소스로서, Apache Iceberg를 사용할 때 엔지니어가 데이터의 버전을 **잘** 관리할 수 있도록 UI 및 API를 제공한다.

Iceberg는 쿼리 엔진이 아니기 때문에, Hive의 알맞은 비교 대상으로 보긴 어렵지만 굳이 차이를 정리해보자면 다음과 같다.

#### Hive vs Iceberg

| - | Hive | Iceberg |
| :---: | --- | --- |
| 유형 | Data Warehouse | Data Lake 용 테이블 포맷 |
| 목적 | 구조화된 데이터를 저장하고 처리하기 위한 플랫폼 | Data Lake에서 신뢰성 있고 효율적인 테이블 관리를 제공 |
| 데이터 저장 | HDFS, S3, GCS 등과 같은 분산 파일 시스템 | 동일 |
| 지원 포맷 | Text, ORC, Parquet Avro | 동일 |
| 메타데이터 저장 | Hive Metastore(RDB) | HDFS, S3 등 Object storage에 저장 |
| 카탈로그 | Hive Metastore | Hive Metastore, AWS Glue, Nessie 등 사용 가능 |
| 데이터 업데이트 | 데이터 삽입, 삭제 시 파일 전체를 다시 작성해야 함 | 데이터 삽입, 삭제 및 병합 작업이 효율적으로 수행 가능 |
| 분산 환경 | Hive Metastore와 MapReduce 중심 | Spark, Flink, Presto와 같은 분산 처리 엔진과 호환 가능 |
| 타임 트래블 | 지원 안 함 | 지원 (과거 스냅샷 조회 가능) |
| 병렬 작업 | 제한적 (충돌 방지 기능이 부족함) | 멀티 브랜칭 및 병합 작업 지원 (Nessie와의 연동으로 더욱 강력해짐) |
| 분석 작업 | 전통적인 배치 처리에 적합 | 실시간 및 배치 처리 모두에 적합 |

## Practice

> 다음 Github 링크에 상세한 설정을 정리해 두었습니다.
> - [https://github.com/ivoryRabbit/play-data-with-docker/tree/master/nessie](https://github.com/ivoryRabbit/play-data-with-docker/tree/master/nessie){: target="_blank"}
{: .prompt-tip }

이제 Docker를 이용해 Iceberg와 Nessie를 테스트할 수 있는 환경을 만들어보자.

우선 Object Storage로는 MinIO를 세팅한다.

```yaml
minio:
    container_name: minio
    image: minio/minio
    ports:
        - "9000:9000"
        - "9001:9001"
    environment:
        MINIO_ROOT_USER: minio
        MINIO_ROOT_PASSWORD: minio123
        MINIO_DOMAIN: minio
    volumes:
        - ./docker/volume/minio:/data
    command: ["server", "/data", "--console-address", ":9001"]
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      retries: 3
      start_period: 5s
    restart: unless-stopped

minio-client:
    container_name: minio-client
    image: minio/mc
    entrypoint: >
      /bin/bash -c "
        mc config --quiet host add storage http://minio:9000 minio minio123 || true;
        mc mb --quiet --ignore-existing storage/hive || true;
        mc mb --quiet --ignore-existing storage/iceberg || true;
      "
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_REGION: ap-northeast-2
      AWS_DEFAULT_REGION: ap-northeast-2
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: true
    depends_on:
      minio:
        condition: service_healthy
    restart: "no"
```

다음은 카탈로그 역할을 할 Nessie이다. 백엔드로 Postgres, MongoDB 등을 사용할 수 있지만 로컬 개발용 In-memory 기능도 지원한다.

```yaml
nessie:
    container_name: nessie
    image: ghcr.io/projectnessie/nessie:0.101.3
    ports:
      - "19120:19120"
      - "9091:9000"
    environment:
      - nessie.version.store.type=IN_MEMORY
    restart: unless-stopped
```

마지막으로 쿼리 엔진은 Trino 혹은 Dremio를 선택한다. Trino의 경우 `/etc/trino/catalog` 경로에서 Iceberg connector와 Catalog type을 구성할 수 있다.

#### [iceberg.properties]

```conf
connector.name=iceberg

iceberg.catalog.type=nessie
# Trino supports Nessie API V2 as of 450
iceberg.nessie-catalog.uri=http://nessie:19120/api/v2
iceberg.nessie-catalog.ref=main
iceberg.nessie-catalog.default-warehouse-dir=s3://iceberg

fs.native-s3.enabled=true
s3.endpoint=http://minio:9000
s3.region=ap-northeast-2
s3.aws-access-key=minio
s3.aws-secret-key=minio123
s3.path-style-access=true
```

```yaml
trino:
    container_name: trino
    hostname: trino
    image: trinodb/trino:450
    ports:
      - "543:543"
    volumes:
      - ./docker/trino/etc:/etc/trino
      - ./docker/volume/trino:/var/lib/trino/data
    depends_on:
      hive-metastore:
        condition: service_healthy
      nessie:
        condition: service_started
    restart: unless-stopped
```

만약 Apache Dremio를 사용한다면, Web에서 Nessie를 등록할 수 있다. 참고로 `debug.addDefaultUser=true` 설정을 키면 Web에 접근할 때 Default credential을 사용할 수 있다.

- Username: dremio
- Password: dremio123

```yaml
dremio:
    container_name: dremio
    image: dremio/dremio-oss:latest
    ports:
      - "9047:9047"
      - "31010:31010"
      - "32010:32010"
    environment:
      - DREMIO_JAVA_SERVER_EXTRA_OPTS=-Dpaths.dist=file:///opt/dremio/data/dis -Ddebug.addDefaultUser=true
    volumes:
      - ./docker/volume/dremio:/op/dremio/data
```

![image_01](/assets/img/posts/2025-01-04/image_01.png){: width="800" }

![image_02](/assets/img/posts/2025-01-04/image_02.png){: width="800" }

마지막으로 1개의 Coordinator와 2개의 Worker로 구성된 Trino Cluster를 세팅한다. Coordinator와 Worker는 동일한 도커 이미지를 사용하되 config.properties 파일을 서로 다르게 구성하여 volume에 마운트하면 된다. 이 때 config.properties 파일 속 설정에 대해서는 반드시 알아야 할 것들이 많으므로 [공식 문서](https://trino.io/docs/current/admin/properties.html){: target="_blank"}를 읽어보는 것이 좋다.

```yaml
trino-1:
    container_name: trino-1
    hostname: trino
    image: trinodb/trino:435
    ports:
      - "8081:8080"
    volumes:
      - ./docker/trino/etc-coordinator:/etc/trino
      - ./docker/trino/catalog:/etc/trino/catalog
    depends_on:
      - hive-metastore

trino-1-worker-1:
    container_name: trino-1-worker-1
    hostname: trino-worker-1
    image: trinodb/trino:435
    ports:
      - "8082:8080"
    volumes:
      - ./docker/trino/etc-worker:/etc/trino
      - ./docker/trino/catalog:/etc/trino/catalog
    depends_on:
      - trino-1
  
trino-1-worker-2:
    container_name: trino-1-worker-2
    hostname: trino-worker-2
    image: trinodb/trino:435
    ports:
      - "8083:8080"
    volumes:
      - ./docker/trino/etc-worker:/etc/trino
      - ./docker/trino/catalog:/etc/trino/catalog
    depends_on:
      - trino-1
```

### 2. Trino Gateway

Trino Gateway 서버는 JVM 기반으로 작동한다. Maven에 등록된 JAR 파일을 다운로드 받은 후 서버를 실행 시킨 뒤, 3개의 port를 열어주어야 한다. 이 때 gateway-config.yaml 파일에는 백엔드로 사용할 Postgres 서버 정보를 입력해 주면 된다.

#### [Dockerfile]

```Dockerfile
FROM openjdk:17-jdk-slim

WORKDIR /etc/trino-gateway

RUN apt-get -y update && apt-get -y install curl

ARG VERSION
ARG JAR_FILE

RUN curl https://repo1.maven.org/maven2/io/trino/gateway/gateway-ha/${VERSION}/gateway-ha-${VERSION}-jar-with-dependencies.jar -o ${JAR_FILE}
```

```yaml
trino-gateway:
    container_name: trino-gateway
    hostname: trino-gateway
    build:
        args:
            VERSION: 4
            JAR_FILE: gateway-ha.jar
        dockerfile: ./docker/trino-gateway/Dockerfile
    image: trino-gateway
    ports:
        - "9080:9080"
        - "9081:9081"
        - "9082:9082"
    volumes:
        - ./docker/trino-gateway/gateway-config.yaml:/etc/trino-gateway/gateway-config.yaml
        - ./docker/trino-gateway/routing-rule.yaml:/etc/trino-gateway/routing-rule.yaml
    depends_on:
        - postgres
    entrypoint: [
        "java", 
        "--add-opens=java.base/java.lang=ALL-UNNAMED", 
        "--add-opens=java.base/java.net=ALL-UNNAMED",
        "-jar", "gateway-ha.jar", 
        "server", "gateway-config.yaml"
    ]
```

Docker compose 실행에 필요한 yaml 파일이 준비되었다면 다음 명령어를 통해 컨테이너들을 생성할 수 있다.

```shell
dodcker compose up #  또는 docker-compose up
```

### 3. Rest API

Trino Cluster와 Trino Gateway 서버가 무사히 띄워진 후에는 Rest API를 이용해 클러스터를 서버에 등록할 수 있다. 또 query parameter를 조작하여 등록과 삭제가 가능하며 등록한 Trino Cluster를 비활성화 시키는 것도 가능하다.

#### [register-trino-1.json]

```json
{
    "name": "trino-1",
    "proxyTo": "http://trino-1:8080",
    "active": true,
    "externalUrl": "http://localhost:8081",
    "routingGroup": "adhoc"
}
```

```bash
curl -H "Content-Type: application/json" \
    -X POST localhost:9080/gateway/backend/modify/update \
    -d @scripts/register-trino-1.json
```

웹 서버(http://localhost:9080/viewgateway)로 접속하면 등록된 클러스터를 확인할 수 있다.

![image_02](/assets/img/posts/2024-01-21/image_02.png){: width="800" height="400" }

앞서 설명한대로 Routing Rule을 구성하면 사용자 별로 서로 다른 클러스터로 라우팅할 수 있다. 예를 들어 "airflow" 라는 계정으로 SQL을 날리면 Routing Group이 "etl"인 클러스터에서 쿼리가 실행되도록 할 수 있다. 또한 yaml 파일만 수정하면 서버를 내리지 않고도 언제든지 라우팅 룰을 추가 및 변경할 수 있다.

#### [routing-rule.yaml]

```yaml
name: "airflow"
description: "if query from airflow, route to etl group"
condition: 'request.getHeader("X-Trino-User") == "airflow"'
actions:
    - 'result.put("routingGroup", "etl")'
```

Trino UI(http://localhost:8080/ui)에서 쿼리 내역을 살펴보면 "admin" 유저는 "trino-1" 클러스터로, "airflow" 유저는 "trino-2" 클러스터로 라우팅되었음을 확인할 수 있다.

![image_03](/assets/img/posts/2024-01-21/image_03.png){: width="800" height="400" }

